#!/usr/bin/env python3
"""
DivineSense æ–‡æ¡£ç®¡ç†è¾…åŠ©è„šæœ¬ v3.0

âš ï¸ **å·²å¼ƒç”¨** - æ­¤è„šæœ¬ä»£è¡¨è¿‡åº¦è„šæœ¬åŒ–çš„åæ¨¡å¼

**è¿ç§»è·¯å¾„**ï¼š
- v3.0 (è„šæœ¬é©±åŠ¨) â†’ v4.0 (AI é©±åŠ¨)
- æ‰€æœ‰é€»è¾‘å·²è¿ç§»åˆ° SKILL.md çš„ system prompt
- AI ç°åœ¨ç›´æ¥ä½¿ç”¨ Glob/Grep/Read å·¥å…·å®Œæˆä»»åŠ¡

**ä¿ç•™åŸå› **ï¼š
- ä½œä¸ºå‚è€ƒå®ç°å±•ç¤º"ä¸æ¨è"çš„è®¾è®¡
- å•å…ƒæµ‹è¯•ä»å¯ç”¨äºéªŒè¯ AI çš„å¼•ç”¨æ£€æµ‹é€»è¾‘
- å¦‚éœ€å¿«é€Ÿæ‰¹é‡æ“ä½œï¼Œå¯æ‰‹åŠ¨è°ƒç”¨

**æ¨èä½¿ç”¨æ–¹å¼**ï¼š
- é€šè¿‡ Claude Code ä½¿ç”¨ `/docs-check`ã€`docs-ref` ç­‰å‘½ä»¤
- è®© AI æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€å†³å®šæ‰§è¡Œç­–ç•¥
"""

import argparse
import json
import logging
import os
import re
import sys
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
import difflib

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s: %(message)s'
)
logger = logging.getLogger(__name__)

# å¼•ç”¨æ£€æµ‹å…³é”®è¯å¸¸é‡
# è¯¦è§ã€å‚è€ƒã€æŸ¥çœ‹ - å¸¸è§ä¸­æ–‡å¼•ç”¨å…³é”®è¯
CHINESE_REF_KEYWORDS = "è¯¦è§å‚è€ƒæŸ¥çœ‹"
ENGLISH_REF_KEYWORDS = "see refer to"


def find_project_root() -> Path:
    """é€šè¿‡æ ‡è®°æ–‡ä»¶å®šä½é¡¹ç›®æ ¹ç›®å½•

    æŒ‰ä¼˜å…ˆçº§æŸ¥æ‰¾ä»¥ä¸‹æ ‡è®°æ–‡ä»¶:
    1. .git - Git ä»“åº“æ ¹ç›®å½•
    2. go.mod - Go é¡¹ç›®æ ¹ç›®å½•
    3. CLAUDE.md - DivineSense ç‰¹æœ‰æ–‡ä»¶

    Returns:
        Path: é¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„

    Raises:
        æ— å¼‚å¸¸ï¼Œæœ€å¤šå‘ä¸ŠæŸ¥æ‰¾ 10 å±‚ï¼Œé™çº§ä½¿ç”¨å›ºå®šæ·±åº¦
    """
    # ä»å½“å‰è„šæœ¬å¼€å§‹å‘ä¸ŠæŸ¥æ‰¾
    current = Path(__file__).resolve().parent
    max_iterations = 10  # é˜²æ­¢æ— é™å¾ªç¯

    for _ in range(max_iterations):
        # æ£€æŸ¥å½“å‰ç›®å½•æ˜¯å¦æ˜¯é¡¹ç›®æ ¹ï¼ˆæœ‰ .git æˆ– go.modï¼‰
        if (current / ".git").exists():
            return current
        if (current / "go.mod").exists():
            return current
        if (current / "CLAUDE.md").exists():  # DivineSense ç‰¹æœ‰æ–‡ä»¶
            return current

        # å‘ä¸Šä¸€å±‚
        current = current.parent

    # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨å›ºå®šæ·±åº¦
    # .claude/skills/docs-manager/docs_helper.py â†’ é¡¹ç›®æ ¹ = 4 å±‚å‘ä¸Š
    return Path(__file__).parent.parent.parent.parent


PROJECT_ROOT = find_project_root()
DOCS_DIR = PROJECT_ROOT / "docs"

logger.info(f"é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT}")
logger.info(f"æ–‡æ¡£ç›®å½•: {DOCS_DIR}")


@dataclass
class Reference:
    """æ–‡æ¡£å¼•ç”¨"""
    source: str      # å¼•ç”¨æºæ–‡ä»¶
    target: str      # è¢«å¼•ç”¨æ–‡ä»¶
    line: int        # è¡Œå·
    ref_type: str    # å¼•ç”¨ç±»å‹
    context: str     # ä¸Šä¸‹æ–‡


@dataclass
class DocNode:
    """æ–‡æ¡£èŠ‚ç‚¹"""
    path: str
    references: List[dict] = None
    referenced_by: List[str] = None

    def __post_init__(self):
        if self.references is None:
            self.references = []
        if self.referenced_by is None:
            self.referenced_by = []


# æ’é™¤ç›®å½•å¸¸é‡
EXCLUDED_DIRS = {
    "node_modules", ".git", ".github",
    "dist", "build", "target", "bin", "obj",
    ".vscode", ".idea", "vendor",
}


def glob_docs(pattern: str = "**/*.md") -> List[Path]:
    """æ‰«ææ–‡æ¡£æ–‡ä»¶ï¼Œæ’é™¤ä¸éœ€è¦çš„ç›®å½•

    Args:
        pattern: glob åŒ¹é…æ¨¡å¼ï¼Œé»˜è®¤ "**/*.md"

    Returns:
        List[Path]: æ–‡æ¡£æ–‡ä»¶åˆ—è¡¨ï¼ˆæ’é™¤ node_modules, .git ç­‰ç›®å½•ï¼‰
    """
    docs = []
    for doc in DOCS_DIR.rglob(pattern):
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œè·¯å¾„æ£€æŸ¥
        doc_str = str(doc)

        # æ’é™¤ç‰¹å®šç›®å½•
        if any(excluded in doc_str for excluded in EXCLUDED_DIRS):
            continue

        # æ’é™¤éšè—æ–‡ä»¶/ç›®å½• (ä»¥ . å¼€å¤´)
        if any(part.startswith('.') for part in doc.parts):
            continue

        docs.append(doc)
    return docs


def extract_references(file_path: Path) -> List[Reference]:
    """ä»æ–‡ä»¶ä¸­æå–æ‰€æœ‰æ–‡æ¡£å¼•ç”¨

    æ”¯æŒçš„å¼•ç”¨æ ¼å¼:
    - Markdown: [æ–‡å­—](docs/xxx.md)
    - @ è¯­æ³•: @docs/xxx.md
    - å¹³é“º: è¯¦è§ docs/xxx.md / see docs/xxx.md
    - URL: https://github.com/.../docs/xxx.md

    Args:
        file_path: è¦åˆ†æçš„æ–‡æ¡£æ–‡ä»¶è·¯å¾„

    Returns:
        List[Reference]: å¼•ç”¨å¯¹è±¡åˆ—è¡¨ï¼ŒåŒ…å«æºæ–‡ä»¶ã€ç›®æ ‡ã€è¡Œå·ã€ç±»å‹ç­‰ä¿¡æ¯
    """
    references = []

    try:
        content = file_path.read_text(encoding="utf-8")
    except PermissionError:
        logger.warning(f"æ— æƒé™è¯»å–: {file_path}")
        return references
    except UnicodeDecodeError:
        logger.warning(f"ç¼–ç é”™è¯¯: {file_path}")
        return references
    except Exception as e:
        logger.error(f"è¯»å–å¤±è´¥ {file_path}: {e}")
        return references

    lines = content.split("\n")

    # æ”¹è¿›çš„å¼•ç”¨æ­£åˆ™æ¨¡å¼
    # æ„å»ºä¸­æ–‡+è‹±æ–‡å…³é”®è¯æ¨¡å¼ï¼Œä½¿ç”¨å¸¸é‡ä¾¿äºç»´æŠ¤
    ref_keywords = f"[{CHINESE_REF_KEYWORDS}]|{ENGLISH_REF_KEYWORDS.replace(' ', '|')}"

    patterns = [
        # Markdown é“¾æ¥
        (r"\[([^\]]+)\]\((docs/[^)]+\.md)\)", "markdown"),
        (r"\[([^\]]+)\]\(\.\./(docs/[^)]+\.md)\)", "markdown"),
        # @ è¯­æ³•
        (r"@docs/[\w/-]+\.md", "at_syntax"),
        # ç»å¯¹ URL
        (r"https://github\.com/[^/]+/[^/]+/docs/[\w/-]+\.md", "absolute_url"),
        # ä»£ç æ³¨é‡Š - ä¸­æ–‡(è¯¦è§å‚è€ƒæŸ¥çœ‹) + è‹±æ–‡(see/refer to)
        (rf"(?:{ref_keywords})\s+[`'\"()]?docs/[\w/-]+\.md", "plain"),
    ]

    for line_no, line in enumerate(lines, 1):
        for pattern, ref_type in patterns:
            try:
                for match in re.finditer(pattern, line, re.IGNORECASE):
                    # æ ¹æ®å¼•ç”¨ç±»å‹æå–ç›®æ ‡
                    if ref_type == "markdown":
                        # Markdown é“¾æ¥: group(2) æ˜¯è·¯å¾„
                        if match.lastindex >= 2:
                            target = match.group(2)
                        else:
                            continue
                    elif ref_type == "at_syntax":
                        target = match.group(0).replace("@", "")
                    elif ref_type == "plain":
                        # æå– docs/xxx.md éƒ¨åˆ†
                        full_match = match.group(0)
                        doc_match = re.search(r"docs/[\w/-]+\.md", full_match)
                        if doc_match:
                            target = doc_match.group(0)
                        else:
                            continue
                    elif ref_type == "absolute_url":
                        url = match.group(0)
                        target = "/docs/" + url.split("/docs/")[-1]
                    else:
                        target = match.group(0)

                    references.append(Reference(
                        source=str(file_path.relative_to(PROJECT_ROOT)),
                        target=target,
                        line=line_no,
                        ref_type=ref_type,
                        context=line.strip()[:80]
                    ))
            except Exception as e:
                logger.debug(f"æ­£åˆ™åŒ¹é…å¤±è´¥: {e}")

    return references


def build_reference_graph() -> Dict[str, DocNode]:
    """æ„å»ºæ–‡æ¡£å¼•ç”¨å…³ç³»å›¾

    æ‰«ææ‰€æœ‰æ–‡æ¡£ï¼Œæå–å¼•ç”¨å…³ç³»ï¼Œæ„å»ºåŒå‘å¼•ç”¨å›¾:
    - references: è¯¥æ–‡æ¡£å¼•ç”¨çš„å…¶ä»–æ–‡æ¡£
    - referenced_by: å“ªäº›æ–‡æ¡£å¼•ç”¨äº†è¯¥æ–‡æ¡£

    Returns:
        Dict[str, DocNode]: ä»¥æ–‡æ¡£è·¯å¾„ä¸ºé”®çš„å¼•ç”¨å›¾
    """
    graph = {}
    docs = glob_docs()

    logger.info(f"æ‰«æ {len(docs)} ä¸ªæ–‡æ¡£...")

    for doc_file in docs:
        try:
            rel_path = str(doc_file.relative_to(DOCS_DIR))
            node = DocNode(path=rel_path)

            refs = extract_references(doc_file)
            for ref in refs:
                node.references.append({
                    "target": ref.target,
                    "type": ref.ref_type,
                    "line": ref.line,
                    "context": ref.context
                })

            graph[rel_path] = node
        except Exception as e:
            logger.warning(f"å¤„ç†æ–‡æ¡£å¤±è´¥ {doc_file}: {e}")

    # æ„å»ºåå‘å¼•ç”¨
    for path, node in graph.items():
        for ref in node.references:
            target = ref["target"]
            # æ ‡å‡†åŒ–è·¯å¾„
            if target.startswith("docs/"):
                target = target[5:]
            elif target.startswith("../docs/"):
                target = target[8:]
            elif target.startswith("/docs/"):
                target = target[6:]

            if target in graph:
                if path not in graph[target].referenced_by:
                    graph[target].referenced_by.append(path)

    return graph


def check_links() -> Dict[str, List[str]]:
    """æ£€æŸ¥æ–‡æ¡£é“¾æ¥æœ‰æ•ˆæ€§

    æ„å»ºå¼•ç”¨å›¾å¹¶éªŒè¯æ¯ä¸ªå¼•ç”¨çš„ç›®æ ‡æ˜¯å¦å­˜åœ¨ã€‚

    Returns:
        Dict[str, List[str]]: æ–­é“¾ä¿¡æ¯å­—å…¸
            - broken_links: æ–­é“¾åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« source, line, target, type
    """
    issues = defaultdict(list)
    graph = build_reference_graph()
    existing_docs = set(graph.keys())

    for path, node in graph.items():
        for ref in node.references:
            target = ref["target"]
            # æ ‡å‡†åŒ–
            if target.startswith("docs/"):
                target = target[5:]
            elif target.startswith("../docs/"):
                target = target[8:]
            elif target.startswith("/docs/"):
                target = target[6:]

            if target not in existing_docs:
                issues["broken_links"].append({
                    "source": path,
                    "line": ref["line"],
                    "target": target,
                    "type": ref["type"]
                })

    return dict(issues)


def get_next_spec_id(phase: int, team: str) -> str:
    """ç”Ÿæˆä¸‹ä¸€ä¸ª Spec ID

    æ‰«ææŒ‡å®š phase å’Œ team ç›®å½•ï¼Œæ‰¾å‡ºæœ€å¤§çš„ ID å·å¹¶åŠ ä¸€ã€‚

    Args:
        phase: Sprint é˜¶æ®µ (1, 2, 3)
        team: å›¢é˜Ÿæ ‡è¯† ("a", "b", "c")

    Returns:
        str: æ ¼å¼ä¸º P{phase}-{team}{åºå·:03d} çš„ Spec ID
    """
    pattern = f"P{phase}-{team}*.md"
    team_dir = DOCS_DIR / "specs" / f"phase-{phase}" / f"team-{team}"

    if not team_dir.exists():
        return f"P{phase}-{team}001"

    existing = list(team_dir.glob(pattern))

    if not existing:
        return f"P{phase}-{team}001"

    max_id = 0
    for f in existing:
        match = re.search(rf"P{phase}-{team}(\d+)", f.stem)
        if match:
            max_id = max(max_id, int(match.group(1)))

    return f"P{phase}-{team}{max_id + 1:03d}"


def detect_duplicates_fast(threshold: float = 0.85) -> List[Tuple[str, str, float]]:
    """å¿«é€Ÿæ£€æµ‹é‡å¤å†…å®¹ - ä»…æ£€æŸ¥å‰ 1000 ä¸ªå­—ç¬¦

    Args:
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œé»˜è®¤ 0.85 (85%)ï¼Œé™ä½ä¼šäº§ç”Ÿæ›´å¤šè¯¯æŠ¥
    """
    duplicates = []
    docs = glob_docs()
    contents = {}

    # è¿‡æ»¤å’Œé¢„å¤„ç†
    for doc in docs:
        if "archived" in str(doc) or "node_modules" in str(doc):
            continue
        try:
            content = doc.read_text(encoding="utf-8", errors="ignore")
            # åªå–å‰ 1000 å­—ç¬¦å¿«é€Ÿæ£€æµ‹
            preview = content[:1000]
            lines = [l.strip() for l in preview.split("\n") if l.strip()]
            contents[doc] = " ".join(lines)
        except Exception:
            pass

    # ä¸¤ä¸¤æ¯”è¾ƒ (ä»…é¢„è§ˆ)
    doc_list = list(contents.items())
    for i in range(len(doc_list)):
        for j in range(i + 1, len(doc_list)):
            doc1, content1 = doc_list[i]
            doc2, content2 = doc_list[j]

            similarity = difflib.SequenceMatcher(None, content1, content2).ratio()

            if similarity >= threshold:
                duplicates.append((
                    str(doc1.relative_to(DOCS_DIR)),
                    str(doc2.relative_to(DOCS_DIR)),
                    similarity
                ))

    return sorted(duplicates, key=lambda x: -x[2])


def classify_document(file_path: Path) -> Tuple[str, str]:
    """æ™ºèƒ½åˆ†ç±»æ–‡æ¡£

    æ ¹æ®æ–‡ä»¶åå’Œè·¯å¾„åˆ¤æ–­æ–‡æ¡£ç±»å‹:
    - core: 00- å¼€å¤´çš„æ ¸å¿ƒè·¯çº¿å›¾
    - reports: *-research.md ç ”ç©¶æŠ¥å‘Š
    - roadmaps: *-roadmap.md è·¯çº¿å›¾
    - practices: PRACTICE* æœ€ä½³å®è·µ
    - Phase X: specs/phase-X/team-Y/ è§„æ ¼

    Args:
        file_path: æ–‡æ¡£è·¯å¾„ï¼ˆç›¸å¯¹äº DOCS_DIRï¼‰

    Returns:
        Tuple[str, str]: (åˆ†ç±», æè¿°)
    """
    name = file_path.name
    rel_path = str(file_path.relative_to(DOCS_DIR))

    if name.startswith("00-"):
        return "core", "æ ¸å¿ƒè·¯çº¿å›¾"
    if name.endswith("-research.md"):
        return "reports", "ç ”ç©¶æŠ¥å‘Š"
    if name.endswith("-roadmap.md"):
        return "roadmaps", "è·¯çº¿å›¾"
    if "PRACTICE" in name or name == "DEBUG_LESSONS.md":
        return "practices", "æœ€ä½³å®è·µ"

    spec_match = re.match(r"phase-(\d)/team-([abc])/", rel_path)
    if spec_match:
        phase, team = spec_match.groups()
        return f"Phase {phase}", f"Team {team.upper()}"

    return "other", "å…¶ä»–"


def print_refs_table(graph: Dict[str, DocNode]):
    """æ‰“å°å¼•ç”¨å…³ç³»è¡¨"""
    # æŒ‰è¢«å¼•ç”¨æ¬¡æ•°æ’åº
    hot_docs = sorted(
        [(path, len(node.referenced_by)) for path, node in graph.items()],
        key=lambda x: -x[1]
    )[:10]

    print("\nğŸ”¥ çƒ­é—¨æ–‡æ¡£ (è¢«å¼•ç”¨æœ€å¤š):")
    for path, count in hot_docs:
        if count > 0:
            print(f"  {count:2d} â† {path}")


def main():
    """ä¸»å…¥å£"""
    parser = argparse.ArgumentParser(description="DivineSense æ–‡æ¡£ç®¡ç†å·¥å…·")
    parser.add_argument("command", nargs="?", default="check",
                       choices=["check", "refs", "next-spec", "duplicates", "tree"],
                       help="å‘½ä»¤")
    parser.add_argument("--json", action="store_true", help="JSON è¾“å‡º")
    parser.add_argument("--phase", type=int, help="Phase (next-spec)")
    parser.add_argument("--team", type=str, help="Team (next-spec)")

    args = parser.parse_args()
    command = args.command

    if command == "check":
        print("ğŸ“‹ æ–‡æ¡£æ£€æŸ¥æŠ¥å‘Š")
        print("=" * 50)

        issues = check_links()
        broken = issues.get("broken_links", [])

        if args.json:
            print(json.dumps(broken, indent=2, ensure_ascii=False))
        elif broken:
            print(f"\nâœ— æ–­é“¾ ({len(broken)}):")
            for issue in broken:
                src = issue['source']
                line = issue['line']
                tgt = issue['target']
                print(f"  ğŸ”— {src}:{line} â†’ {tgt}")
        else:
            print("\nâœ“ æ— æ–­é“¾")

    elif command == "refs":
        print("ğŸ”— å¼•ç”¨å…³ç³»å›¾")
        print("=" * 50)

        graph = build_reference_graph()

        if args.json:
            # è½¬æ¢ä¸º JSON å¯åºåˆ—åŒ–æ ¼å¼
            output = {}
            for path, node in graph.items():
                output[path] = {
                    "references": node.references,
                    "referenced_by": node.referenced_by
                }
            print(json.dumps(output, indent=2, ensure_ascii=False))
        else:
            for path, node in sorted(graph.items()):
                if node.references or node.referenced_by:
                    print(f"\n{path}:")
                    print(f"  å¼•ç”¨: {len(node.references)} ä¸ª")
                    print(f"  è¢«å¼•ç”¨: {len(node.referenced_by)} æ¬¡")

            print_refs_table(graph)

    elif command == "next-spec":
        phase = args.phase or 2
        team = args.team or "a"
        spec_id = get_next_spec_id(phase, team)

        if args.json:
            print(json.dumps({"spec_id": spec_id, "phase": phase, "team": team}))
        else:
            print(f"ğŸ“„ ä¸‹ä¸€ä¸ª Spec ID: {spec_id}")

    elif command == "duplicates":
        print("ğŸ” é‡å¤å†…å®¹æ£€æµ‹")
        print("=" * 50)

        dupes = detect_duplicates_fast()

        if args.json:
            print(json.dumps(dupes, indent=2))
        elif dupes:
            for doc1, doc2, sim in dupes[:10]:
                print(f"\n{sim:.1%} ç›¸ä¼¼åº¦:")
                print(f"  1. {doc1}")
                print(f"  2. {doc2}")
        else:
            print("\nâœ“ æ— é‡å¤å†…å®¹")

    elif command == "tree":
        print("ğŸ“‚ docs/ ç»“æ„")
        print("=" * 50)

        for item in sorted(DOCS_DIR.iterdir()):
            if item.is_dir() and not item.name.startswith("."):
                count = len(list(item.rglob("*.md")))
                print(f"ğŸ“ {item.name}/ ({count} ä¸ª md æ–‡ä»¶)")
            elif item.suffix == ".md":
                print(f"ğŸ“„ {item.name}")


if __name__ == "__main__":
    main()
