# ==============================================================================
# DivineSense 环境变量配置
# ==============================================================================
# 使用: cp .env.example .env && 编辑填入配置

# ==============================================================================
# [必填] 最小启动配置
# ==============================================================================

# 数据库 (AI 功能需要 PostgreSQL + pgvector)
# 开发环境默认配置 (匹配 docker/compose/dev.yml)
DIVINESENSE_DRIVER=postgres
DIVINESENSE_DSN=postgres://divinesense:divinesense@localhost:25432/divinesense?sslmode=disable

# 生产环境部署时，请修改为实际配置
# DIVINESENSE_DSN=postgres://divinesense:<your-password>@localhost:5432/divinesense?sslmode=disable

# AI 功能
DIVINESENSE_AI_ENABLED=true
DIVINESENSE_AI_SILICONFLOW_API_KEY=sk-your-siliconflow-key
DIVINESENSE_AI_DEEPSEEK_API_KEY=sk-your-deepseek-key

# ==============================================================================
# [可选] Geek Mode - Claude Code CLI 集成
# ==============================================================================
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ Geek Mode 允许用户在聊天中调用 Claude Code CLI 处理通用运维与开发任务          │
# │                                                                             │
# │ 核心能力 (不止于编码)：                                                       │
# │   • 💻 代码开发: 审查、修复 Bug、重构、单元测试、API 实现                     │
# │   • 🔍 系统运维: 日志分析、进程监控、网络排查、配置文件修改                    │
# │   • 📂 文件管理: 批量处理、格式转换、搜索替换、目录整理                        │
# │   • 🗄️ 数据维护: 数据库查询、备份恢复、数据迁移、ETL 脚本                     │
# │   • 🔄 自动化:  编写 Shell/Python 脚本、Git 操作、CI/CD 辅助                   │
# │                                                                             │
# │ 工作流程：                                                                   │
# │   1. 用户启用 Geek Mode (UI 开关)                                           │
# │   2. 发送自然语言指令 (不仅仅是关于代码)                                      │
# │   3. 系统自动调用 Claude Code Agent (基于 Shell/Go 插件)                     │
# │   4. 智能执行终端命令并返回结果                                              │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# -----------------------------------------------------------------------------
# 前置条件
# -----------------------------------------------------------------------------
#
# 1. 安装 Claude Code CLI:
#    npm install -g @anthropic-ai/claude-code
#    或参考: https://github.com/anthropics/claude-code
#
# 2. 验证安装:
#    claude --version
#
# 3. 配置认证 (推荐使用智谱适配工具):
#    npx @z_ai/coding-helper
#    或者手动登录: claude auth login
#
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# 环境变量配置
# -----------------------------------------------------------------------------
#
# 启用 Geek Mode:
#   设置此开关为 true 以开启 Geek Mode 功能模块。
#   开启后，用户可在前端界面通过独立开关（Switch）进入 Geek Mode。
#   进入该模式后的所有交互均由 Claude Code Agent 接管。
DIVINESENSE_CLAUDE_CODE_ENABLED=false
#
# Geek Mode 工作目录 (可选):
#   指定 Geek Mode 的沙箱及代码操作的基础目录。
#   每个用户的沙箱将创建在此目录下: {WORKDIR}/user_{id}
#   默认值: ~/.divinesense/claude
# DIVINESENSE_CLAUDE_CODE_WORKDIR=/opt/divinesense/data
##
# 启用 Evolution Mode (进化模式):
#   进化模式是 Geek Mode 的增强版，允许 Agent 自主修改项目核心代码。
#   注意：此模式权限极高，建议仅在受控环境开启。
DIVINESENSE_EVOLUTION_ENABLED=false
#
# Evolution Mode 仅限管理员 (可选):
# DIVINESENSE_EVOLUTION_ADMIN_ONLY=true
#
# Docker 环境特殊说明:
#   • Docker 容器内需要安装 Claude Code CLI
#   • 建议在 Dockerfile 中添加:
#     RUN npm install -g @anthropic-ai/claude-code
#   • 工作目录默认挂载到 /var/opt/divinesense
#   • 确保容器有权限访问代码目录
#
# 本地开发环境:
#   • 直接设置 DIVINESENSE_CLAUDE_CODE_ENABLED=true
#   • 系统会使用当前工作目录作为代码操作目录
#   • 每个用户有独立的会话，文件变更限制为 100 次/会话
#   • 执行超时时间为 5 分钟
#
# -----------------------------------------------------------------------------
# 使用示例
# -----------------------------------------------------------------------------
#
# 1. Geek Mode (沙箱环境 - 独立工作区):
#    "写一个 Python 脚本查询当前外网 IP 并保存到 ip.txt"
#    "帮我分析刚刚上传的 error.log 日志文件"
#    "生成一个 RSA 私钥并打印出来"
#    "列出当前目录下的所有文件"
#
# 2. Evolution Mode (进化模式 - 修改项目源码):
#    "为项目添加一个新的 API 接口: GET /api/v1/ping"
#    "修复 server/main.go 中的启动逻辑 bug"
#    "为 user_service.go 编写单元测试"
#    "检查 git status 并提交当前的更改"
#
# -----------------------------------------------------------------------------
# 安全限制
# -----------------------------------------------------------------------------
#
#   • 每个用户独立会话，相互隔离
#   • 文件变更次数限制: 100 次/会话
#   • 执行超时: 5 分钟
#   • 所有文件操作限定在配置的工作目录内
#
# -----------------------------------------------------------------------------
# 故障排查
# -----------------------------------------------------------------------------
#
# 问题: "Geek Mode is disabled"
# 解决: 设置 DIVINESENSE_CLAUDE_CODE_ENABLED=true
#
# 问题: "Claude Code CLI not found"
# 解决: 确保系统已安装 claude 命令，且在 PATH 中
#
# 问题: Docker 容器内无法使用
# 解决: 在 Dockerfile 中添加 Claude Code CLI 安装步骤
#
# -----------------------------------------------------------------------------

# ==============================================================================
# [可选] AI 模型配置
# ==============================================================================
#
# 系统使用 4 类模型，各司其职：
#
# ┌──────────────┬──────────────────────────────────────────────────────────────┐
# │ 模型类型     │ 用途说明                                                     │
# ├──────────────┼──────────────────────────────────────────────────────────────┤
# │ 对话 (LLM)   │ Agent 核心能力：对话、推理、工具调用                         │
# │              │ 场景：日程创建、笔记问答、综合分析                           │
# ├──────────────┼──────────────────────────────────────────────────────────────┤
# │ 向量 (Embed) │ 文本向量化：将笔记转为向量存入 pgvector                      │
# │              │ 场景：语义搜索 "找关于项目管理的笔记"                        │
# ├──────────────┼──────────────────────────────────────────────────────────────┤
# │ 意图 (Intent)│ 用户输入分类：判断是创建/查询/修改日程                       │
# │              │ 场景：区分 "明天开会" vs "明天有空吗"                        │
# ├──────────────┼──────────────────────────────────────────────────────────────┤
# │ 重排 (Rerank)│ 搜索结果优化：对向量召回结果精排                             │
# │              │ 场景：提升搜索准确率，过滤无关结果                           │
# └──────────────┴──────────────────────────────────────────────────────────────┘
#
# 默认配置 (推荐):
#
# ┌──────────────┬────────────────┬──────────────────────────┬─────────────────┐
# │ 模型类型     │ Provider       │ 模型                     │ 使用的 API Key  │
# ├──────────────┼────────────────┼──────────────────────────┼─────────────────┤
# │ 对话 (LLM)   │ deepseek       │ deepseek-chat            │ DEEPSEEK_API_KEY│
# │ 向量 (Embed) │ siliconflow    │ BAAI/bge-m3              │ SILICONFLOW_KEY │
# │ 意图 (Intent)│ siliconflow    │ Qwen/Qwen2.5-7B-Instruct │ SILICONFLOW_KEY │
# │ 重排 (Rerank)│ siliconflow    │ BAAI/bge-reranker-v2-m3  │ SILICONFLOW_KEY │
# └──────────────┴────────────────┴──────────────────────────┴─────────────────┘
#
# 为什么这样分配？
# - DeepSeek: 对话能力强、推理深、工具调用稳定，适合做主 LLM
# - SiliconFlow: 向量/重排模型全、价格低、国内访问快，适合做基础设施
# - 意图分类用轻量 7B 模型：响应快(<500ms)、成本低、准确率高

# -----------------------------------------------------------------------------
# 自定义 Provider (按需修改)
# -----------------------------------------------------------------------------

# 向量 Provider: siliconflow (默认) | openai | ollama
# DIVINESENSE_AI_EMBEDDING_PROVIDER=siliconflow

# 对话 Provider: deepseek (默认) | openai | ollama | siliconflow
# DIVINESENSE_AI_LLM_PROVIDER=deepseek

# -----------------------------------------------------------------------------
# 自定义模型 (按需修改)
# -----------------------------------------------------------------------------

# 向量模型 (需与 Provider 匹配)
# DIVINESENSE_AI_EMBEDDING_MODEL=BAAI/bge-m3

# 重排模型 (固定使用 SiliconFlow)
# DIVINESENSE_AI_RERANK_MODEL=BAAI/bge-reranker-v2-m3

# 对话模型 (需与 Provider 匹配)
# DIVINESENSE_AI_LLM_MODEL=deepseek-chat

# 意图分类模型 (固定使用 SiliconFlow，无需配置)
# 默认: Qwen/Qwen2.5-7B-Instruct

# ==============================================================================
# [可选] 其他 Provider API Keys
# ==============================================================================

# OpenAI (如需使用 gpt-4o 等模型)
# DIVINESENSE_AI_OPENAI_API_KEY=sk-your-openai-key

# Ollama (本地部署)
# DIVINESENSE_AI_OLLAMA_BASE_URL=http://localhost:11434

# ==============================================================================
# [Chat Apps] 聊天应用集成 - 安全加密配置
# ==============================================================================
#
# 用于加密存储 Telegram, WhatsApp, DingTalk 的访问令牌 (Access Token/App Secret)
# 采用 AES-256-GCM 算法，要求密钥长度必须严格为 32 字节。
#
# ⚠️ 重要警告:
# 1. 如果启用了 Chat Apps 功能，此配置项为 **必需**。
# 2. 生产环境请生成一个独立的随机密钥。
#
# 生成命令:
#   openssl rand -base64 32 | cut -c1-32
#
# 示例:
# DIVINESENSE_CHAT_APPS_SECRET_KEY=change_me_to_a_random_32_byte_string
#
# 实例 URL (必需):
#   用于生成 Webhook URL，供聊天平台回调。
#   开发环境默认: http://localhost:28081
#   生产环境示例: https://your-domain.com
# DIVINESENSE_INSTANCE_URL=http://localhost:28081
#
# WhatsApp 桥接服务地址 (可选):
#   如果使用 WhatsApp，需要部署独立的 Baileys 桥接服务。
#   留空则表示不使用 WhatsApp 功能。
# DIVINESENSE_WHATSAPP_BRIDGE_URL=http://localhost:3001

# ==============================================================================
# [可选] 附件处理
# ==============================================================================

# OCR 图片文字提取 (需安装 Tesseract)
# DIVINESENSE_OCR_ENABLED=true
# DIVINESENSE_OCR_LANGUAGES=chi_sim+eng

# 文档文本提取 (需部署 Apache Tika)
# DIVINESENSE_TEXTEXTRACT_ENABLED=true
# DIVINESENSE_TEXTEXTRACT_TIKA_URL=http://localhost:9998

# ==============================================================================
# 配置方案速查
# ==============================================================================
#
# 方案 A: SiliconFlow + DeepSeek [推荐]
# -------------------------------------
# 成本最优，国内访问稳定
# DIVINESENSE_AI_SILICONFLOW_API_KEY=sk-xxx
# DIVINESENSE_AI_DEEPSEEK_API_KEY=sk-xxx
#
# 方案 B: 纯 SiliconFlow
# ----------------------
# 单一供应商，管理简单
# DIVINESENSE_AI_LLM_PROVIDER=siliconflow
# DIVINESENSE_AI_SILICONFLOW_API_KEY=sk-xxx
# DIVINESENSE_AI_LLM_MODEL=Qwen/Qwen2.5-72B-Instruct
#
# 方案 C: OpenAI 全家桶
# --------------------
# 海外用户，追求效果
# DIVINESENSE_AI_EMBEDDING_PROVIDER=openai
# DIVINESENSE_AI_LLM_PROVIDER=openai
# DIVINESENSE_AI_OPENAI_API_KEY=sk-xxx
# DIVINESENSE_AI_EMBEDDING_MODEL=text-embedding-3-small
# DIVINESENSE_AI_LLM_MODEL=gpt-4o
#
# 方案 D: 本地 Ollama
# ------------------
# 完全离线，数据私密
# DIVINESENSE_AI_EMBEDDING_PROVIDER=ollama
# DIVINESENSE_AI_LLM_PROVIDER=ollama
# DIVINESENSE_AI_OLLAMA_BASE_URL=http://localhost:11434
# DIVINESENSE_AI_EMBEDDING_MODEL=nomic-embed-text
# DIVINESENSE_AI_LLM_MODEL=llama3.1

# ==============================================================================
# [二进制部署] 额外配置
# ==============================================================================
#
# 二进制部署安装位置:
#   - 二进制文件: /opt/divinesense/bin/divinesense
#   - 数据目录:   /opt/divinesense/data
#   - 配置文件:   /etc/divinesense/config
#   - 日志目录:   /opt/divinesense/logs
#
# 快速安装:
#   curl -fsSL https://raw.githubusercontent.com/hrygo/divinesense/main/deploy/aliyun/install.sh | sudo bash -s -- --mode=binary
#
# 详细文档: docs/deployment/BINARY_DEPLOYMENT.md
#
# 二进制部署时:
# 1. 复制此文件到 /etc/divinesense/config
# 2. 确保 DIVINESENSE_DATA 目录可写
# 3. 配置 PostgreSQL (Docker 或系统)
# 4. 运行: sudo systemctl enable --now divinesense


